# 架构决策：限流模式选择与 Nacos 持久化方案

> **创建时间**：2024-12-09  
> **状态**：待确认  
> **目的**：明确项目的限流架构设计和数据持久化方案

---

## 核心问题

### 问题 1：不同机器不同性能怎么办？

**场景**：同一个服务部署在不同性能的机器上，需要不同的限流阈值。

**官方 Sentinel 的设计**：

- 支持**机器级别的规则推送**（V1 API）
- 每个机器（ip:port）可以有独立的规则配置
- Dashboard 可以针对单个机器下发规则

**我们的改进（V2 API + Nacos）**：

- **应用级别的规则管理**（一个 app，一套规则）
- 规则存储在 Nacos，所有实例共享
- 适合 **云原生/K8s 环境**（实例对等）

**冲突分析**：

```
场景：同一服务，3 台机器性能不同
- 机器 A（8 核）：能处理 1000 QPS
- 机器 B（4 核）：能处理 500 QPS
- 机器 C（2 核）：能处理 200 QPS

V1 方案（机器级）：
✅ 可以为每台机器设置不同的阈值
❌ 规则管理复杂（3 台机器 = 3 份配置）
❌ 机器扩缩容后规则需要重新配置
❌ 重启 Dashboard 规则丢失

V2 方案（应用级）：
✅ 规则管理简单（1 个 app = 1 份配置）
✅ Nacos 持久化，重启不丢失
✅ 适合 K8s 自动扩缩容
❌ 无法为单台机器设置特殊阈值
```

**推荐方案**：

| 场景                   | 推荐模式            | 说明                                  |
| ---------------------- | ------------------- | ------------------------------------- |
| K8s/容器化部署（推荐） | **V2 + 应用级规则** | 所有 Pod 性能一致，使用统一规则       |
| 传统物理机/性能不一致  | V1 + 机器级规则     | 为每台机器单独配置                    |
| 需要全局流控           | **集群流控模式**    | 多实例共享总阈值（见问题 3）          |
| 性能差异 + 全局流控    | 集群流控 + 权重     | Token Server 根据权重分配令牌（高级） |

**我们的架构选择**：

- ✅ **优先支持 V2 + Nacos**（面向云原生）
- ✅ **支持集群流控**（全局总量控制）
- ⚠️ **保留 V1 API**（兼容特殊场景，但不推荐）

---

### 问题 2：K8s 环境按服务级别限流

**场景**：K8s 部署，每个 Pod 配置相同，希望所有 Pod 使用相同的限流规则。

**答案**：✅ **这正是 V2 + Nacos 的核心优势！**

#### 工作流程

```
┌─────────────────────────────────────────────────────────┐
│                  Nacos 配置中心                          │
│   Data ID: my-service-flow-rules                        │
│   规则：/api/user → 100 QPS（单机）                      │
└──────────────────┬──────────────────────────────────────┘
                   │ 监听配置变化
         ┌─────────┼─────────┬─────────┐
         │         │         │         │
         ▼         ▼         ▼         ▼
    ┌────────┐┌────────┐┌────────┐┌────────┐
    │Pod-1   ││Pod-2   ││Pod-3   ││Pod-N   │
    │100 QPS ││100 QPS ││100 QPS ││100 QPS │
    └────────┘└────────┘└────────┘└────────┘
```

#### 优势

1. **配置一致性**

   ```yaml
   # 所有 Pod 自动加载相同规则
   # 不需要为每个 Pod 单独配置
   ```

2. **动态更新**

   ```
   Dashboard 修改规则
     ↓
   推送到 Nacos
     ↓
   所有 Pod 自动更新（秒级生效）
   ```

3. **扩缩容友好**

   ```
   K8s HPA 扩容：Pod 1 → Pod 10
     ↓
   新 Pod 启动时自动从 Nacos 拉取规则
     ↓
   无需手动配置
   ```

4. **持久化保障**
   ```
   Pod 重启/重建
     ↓
   从 Nacos 恢复规则
     ↓
   规则永不丢失
   ```

#### 单机 vs 集群模式选择

在 K8s 环境下，有两种限流模式：

**模式 A：单机限流（默认）**

```
配置：/api/user → 100 QPS
实际：每个 Pod 限流 100 QPS
总容量：100 QPS × Pod 数量

示例：
- 10 个 Pod → 总容量 1000 QPS
- 扩容到 20 个 Pod → 总容量 2000 QPS
```

**适用场景**：

- 按实例分配流量（如 CPU/内存密集型）
- 需要水平扩展增加总容量
- 典型的微服务架构

**模式 B：集群限流（需启用）**

```
配置：/api/user → 100 QPS（总量）
实际：所有 Pod 共享 100 QPS 配额
总容量：始终 100 QPS（不随扩容增加）

示例：
- 10 个 Pod → 总容量 100 QPS（每个 Pod 约 10 QPS）
- 扩容到 20 个 Pod → 总容量仍 100 QPS（每个 Pod 约 5 QPS）
```

**适用场景**：

- 下游服务有硬性容量限制（如第三方 API）
- 需要全局总量控制
- 防止突发流量打垮后端

---

### 问题 3：集群流控到底是基于什么控制？

**答案**：集群流控是基于 **Token Server** 实现的**全局令牌桶算法**。

#### 核心原理

```
┌─────────────────────────────────────────────────────────┐
│              Token Server（令牌服务器）                   │
│  ┌──────────────────────────────────────────┐           │
│  │  令牌桶：容量 100，每秒生成 100 个令牌  │           │
│  └──────────────────────────────────────────┘           │
│    Port: 18730（集群通信端口）                          │
└─────────────────┬───────────────────────────────────────┘
                  │ gRPC/HTTP 请求令牌
        ┌─────────┼─────────┬─────────┐
        │         │         │         │
        ▼         ▼         ▼         ▼
   ┌────────┐┌────────┐┌────────┐┌────────┐
   │ Pod-1  ││ Pod-2  ││ Pod-3  ││ Pod-N  │
   │(Client)││(Client)││(Client)││(Client)│
   └────────┘└────────┘└────────┘└────────┘
```

#### 工作流程

1. **规则配置**

   ```json
   {
     "resource": "/api/payment",
     "count": 100, // 全局限流 100 QPS
     "clusterMode": true, // 启用集群模式
     "clusterConfig": {
       "thresholdType": 1 // 1=总体阈值，0=单机均摊
     }
   }
   ```

2. **请求处理流程**

   ```
   用户请求到达 Pod-1
     ↓
   Pod-1 向 Token Server 请求令牌
     ↓
   Token Server 检查令牌桶
     ├─ 有令牌 → 返回 true → 放行请求
     └─ 无令牌 → 返回 false → 拒绝请求（429）
   ```

3. **令牌分配策略**

   **策略 A：总体阈值（推荐）**

   ```
   配置：count = 100, thresholdType = 1

   Token Server 令牌桶：100 个令牌/秒
   任意 Pod 都可以申请，先到先得

   示例：
   - Pod-1 处理 60 个请求 → 消耗 60 令牌
   - Pod-2 处理 30 个请求 → 消耗 30 令牌
   - Pod-3 处理 10 个请求 → 消耗 10 令牌
   - Pod-4 来晚了 → 无令牌 → 被限流
   总计：100 QPS（严格控制）
   ```

   **策略 B：单机均摊**

   ```
   配置：count = 100, thresholdType = 0

   Token Server 计算：100 / 在线 Pod 数

   示例（10 个 Pod）：
   - 每个 Pod 分配 10 个令牌/秒
   - Pod-1 最多处理 10 QPS
   - Pod-2 最多处理 10 QPS
   总计：约 100 QPS（非严格）
   ```

#### Token Server 的三种部署模式

| 模式         | 说明                           | 优点           | 缺点                  | 适用场景         |
| ------------ | ------------------------------ | -------------- | --------------------- | ---------------- |
| **独立部署** | 单独的 StatefulSet/Deployment  | 高可用、易维护 | 需要额外资源          | **生产环境推荐** |
| 嵌入模式     | 从应用 Pod 中选一个作为 Server | 节省资源       | Server Pod 重启影响大 | 测试环境         |
| 单机模式     | 不用 Token Server              | 简单           | 无全局控制能力        | 开发环境         |

#### 本项目的集群流控实现

我们已经实现了**独立部署模式**：

1. **Token Server 容器**

   ```yaml
   # docker-compose.yml
   token-server:
     image: sentinel-token-server
     ports:
       - "8081:8081" # HTTP 管理端口
       - "18730:18730" # 集群通信端口
     environment:
       - CLUSTER_SERVER_PORT=18730
       - NACOS_SERVER_ADDR=nacos:8848
   ```

2. **客户端接入**

   ```java
   // 应用启动时配置
   ClusterClientConfigManager.applyNewAssignConfig(
       new ClusterClientAssignConfig()
           .setServerHost("token-server")
           .setServerPort(18730)
   );
   ```

3. **规则配置**
   - 在 Dashboard UI 创建规则时勾选"是否集群"
   - 规则保存到 Nacos：`${app}-flow-rules`
   - Token Server 和客户端都从 Nacos 加载规则

#### 集群流控的关键控制点

| 控制点         | 位置         | 作用                           |
| -------------- | ------------ | ------------------------------ |
| **全局令牌桶** | Token Server | 管理总的流量配额               |
| **规则配置**   | Nacos        | 存储限流规则（阈值、模式等）   |
| **令牌申请**   | 客户端 Pod   | 每次请求前向 Token Server 申请 |
| **流量统计**   | Token Server | 统计全局 QPS、拒绝率等         |
| **降级策略**   | 客户端 Pod   | Token Server 不可用时本地降级  |

---

## 架构决策总结

### 我们的技术选型

```
┌─────────────────────────────────────────────────────────┐
│                    架构分层                              │
├─────────────────────────────────────────────────────────┤
│  前端层   │ React 19 + Chakra UI（统一使用 V2 API）     │
├─────────────────────────────────────────────────────────┤
│  API 层   │ V2 Controller（应用级）+ V1（兼容）        │
├─────────────────────────────────────────────────────────┤
│  持久化   │ Nacos（所有规则类型）                        │
├─────────────────────────────────────────────────────────┤
│  限流模式 │ 单机模式（默认）+ 集群模式（可选）           │
├─────────────────────────────────────────────────────────┤
│  部署环境 │ K8s/Docker（云原生优先）                    │
└─────────────────────────────────────────────────────────┘
```

### 三个问题的最终答案

#### 1️⃣ 不同机器不同性能怎么办？

**回答**：

- **推荐方案**：统一机器性能（K8s 标准化 Pod）
- **替代方案**：使用集群流控 + 降级策略
- **不推荐**：V1 机器级规则（违背云原生理念）

**理由**：

- 云原生时代，容器化部署应该保证**实例对等**
- 如果确实有性能差异，应该通过 K8s Node Selector/Affinity 隔离
- 限流规则不应该感知底层硬件差异

#### 2️⃣ K8s 按服务级别限流是非常好的？

**回答**：✅ **完全正确！这正是本项目的核心设计目标。**

**实现**：

```
Dashboard UI 配置规则
  ↓
推送到 Nacos（应用级）
  ↓
所有 Pod 自动加载（通过 sentinel-datasource-nacos）
  ↓
配置一致、动态更新、扩缩容友好
```

#### 3️⃣ 集群流控到底基于什么控制？

**回答**：

- **控制机制**：Token Server 的全局令牌桶
- **通信协议**：gRPC/HTTP（端口 18730）
- **令牌策略**：总体阈值（严格）或 单机均摊（宽松）
- **高可用**：支持多 Token Server + 客户端降级

**核心价值**：

- 解决单机限流的"总量失控"问题
- 适合下游容量有限的场景（如第三方 API）
- 典型案例：10 个 Pod，但下游只能承受 100 QPS

---

## 实施方案

### 阶段 1：Nacos 持久化（当前优先级）

**目标**：所有规则类型支持 Nacos 存储

**任务**：

1. ✅ 流控规则（已有示例代码）
2. ⬜ 降级规则（需创建 NacosProvider/Publisher）
3. ⬜ 热点规则（需创建 NacosProvider/Publisher）
4. ⬜ 系统规则（需创建 NacosProvider/Publisher）
5. ⬜ 授权规则（需创建 NacosProvider/Publisher）
6. ⬜ 修改 V2 控制器使用 Nacos（当前用的是 RuleRepository）

**验证标准**：

- Dashboard 重启后规则不丢失
- 多 Pod 环境配置自动同步
- 客户端能从 Nacos 加载规则

### 阶段 2：集群流控（已实现，需完善）

**目标**：生产级的集群流控支持

**当前状态**：

- ✅ Token Server 独立部署（docker-compose）
- ✅ 基础的集群流控功能
- ⬜ 多 Token Server 高可用（未实现）
- ⬜ 客户端降级策略（需完善）

**优化方向**：

1. Token Server 多实例部署
2. 客户端连接池优化
3. 监控指标完善（令牌申请延迟、拒绝率等）

### 阶段 3：API 兼容性（低优先级）

**目标**：保留 V1 API 兼容性（不推荐使用）

**原因**：

- 有些旧系统可能依赖 V1 API
- 作为特殊场景的 fallback

**实现**：

- 保留现有 V1 Controller
- 文档标注"不推荐"
- 新功能只在 V2 实现

---

## 需要确认的决策

### 决策 1：是否完全移除机器级规则管理？

**选项 A：完全移除（激进）**

- ✅ 代码简洁，维护成本低
- ✅ 强制用户遵循云原生最佳实践
- ❌ 可能无法兼容某些特殊场景

**选项 B：保留但不推荐（保守）**

- ✅ 向后兼容，适应性强
- ❌ 代码冗余，维护成本高
- ❌ 用户可能误用

**推荐**：✅ **选项 B（保留 V1，但文档明确不推荐）**

### 决策 2：Nacos 配置的 dataId 命名规范？

**当前规范**（基于代码）：

```java
String dataId = appName + FLOW_DATA_ID_POSTFIX;
// 示例：my-service-flow-rules
```

**建议扩展**：
| 规则类型 | dataId 后缀 | 示例 |
| -------- | ------------------------ | ------------------------------ |
| 流控 | `-flow-rules` | `my-service-flow-rules` |
| 降级 | `-degrade-rules` | `my-service-degrade-rules` |
| 热点 | `-param-flow-rules` | `my-service-param-flow-rules` |
| 系统 | `-system-rules` | `my-service-system-rules` |
| 授权 | `-authority-rules` | `my-service-authority-rules` |

**需要确认**：✅ 是否采用此命名规范？

### 决策 3：集群流控是否作为核心功能推广？

**场景分析**：

- 大部分微服务：单机限流足够（每个 Pod 独立限流）
- 特定场景：需要集群流控（如调用第三方 API）

**推荐**：

- ✅ 支持集群流控（已实现）
- ✅ 默认使用单机模式（简单）
- ✅ 文档说明何时需要集群模式

---

## 下一步行动

### 请确认以下问题：

1. **限流模式**：

   - [ ] 确认优先支持 K8s/容器化环境（应用级规则）
   - [ ] 确认保留 V1 API 兼容性（但不推荐）
   - [ ] 确认集群流控作为可选功能（不是默认）

2. **Nacos 持久化**：

   - [ ] 确认 dataId 命名规范（`${app}-${type}-rules`）
   - [ ] 确认优先实施所有规则类型的 Nacos 支持
   - [ ] 确认 V2 控制器全面切换到 Nacos（移除 RuleRepository）

3. **实施顺序**：
   - [ ] 先完成 Nacos 持久化（5 种规则类型）
   - [ ] 再完善集群流控高可用
   - [ ] 最后优化文档和测试

**确认后我将开始实施！** 🚀
